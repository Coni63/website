{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will setup a model for Sentiment Analysis. To do so, we will use a known dataset named \"sentiment140\". It has been created from 1.6m tweet selected based on a smiley. If the tweet has an \"Happy smiley\" it is supposed to be positive else it's negative. After that smileys are removed. I tell you this point as it will be relevant at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Dataset\n",
    "\n",
    "To do so, we will only keep the text and the target. After based on exploration, we will apply regular expression to clean sentences :\n",
    "- remove hastags\n",
    "- remove names (@foo)\n",
    "- remove urls\n",
    "- merge some vocabulary (for example \"can't\", \"cant\", \"isn't\", \"isnt\")\n",
    "- replace \"...\" by \"three_dots\" to be considered in the model\n",
    "- replace \"!!!!\" by \"exc_mark\" to be considered in the model\n",
    "- remove all repeated character (for example looooooooooooooool = lool != lol) as I keep 2 of them by samefty to not have foot -> fot for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"F:/Twitter_data/dataset/sentiment_140.csv\", encoding='latin1', header=None)\n",
    "df.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      "target    1600000 non-null int64\n",
      "id        1600000 non-null int64\n",
      "date      1600000 non-null object\n",
      "flag      1600000 non-null object\n",
      "user      1600000 non-null object\n",
      "text      1600000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.target /= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "y = df[\"target\"]\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          is upset that he can't update his Facebook by ...\n",
       "2          @Kenichan I dived many times for the ball. Man...\n",
       "3            my whole body feels itchy and like its on fire \n",
       "4          @nationwideclass no, it's not behaving at all....\n",
       "5                              @Kwesidei not the whole crew \n",
       "6                                                Need a hug \n",
       "7          @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "8                       @Tatiana_K nope they didn't have it \n",
       "9                                  @twittera que me muera ? \n",
       "10               spring break in plain city... it's snowing \n",
       "11                                I just re-pierced my ears \n",
       "12         @caregiving I couldn't bear to watch it.  And ...\n",
       "13         @octolinz16 It it counts, idk why I did either...\n",
       "14         @smarrison i would've been the first, but i di...\n",
       "15         @iamjazzyfizzle I wish I got to watch it with ...\n",
       "16         Hollis' death scene will hurt me severely to w...\n",
       "17                                      about to file taxes \n",
       "18         @LettyA ahh ive always wanted to see rent  lov...\n",
       "19         @FakerPattyPattz Oh dear. Were you drinking ou...\n",
       "20         @alydesigns i was out most of the day so didn'...\n",
       "21         one of my friend called me, and asked to meet ...\n",
       "22          @angry_barista I baked you a cake but I ated it \n",
       "23                    this week is not going as i had hoped \n",
       "24                                blagh class at 8 tomorrow \n",
       "25            I hate when I have to call and wake people up \n",
       "26         Just going to cry myself to sleep after watchi...\n",
       "27                                    im sad now  Miss.Lilly\n",
       "28         ooooh.... LOL  that leslie.... and ok I won't ...\n",
       "29         Meh... Almost Lover is the exception... this t...\n",
       "                                 ...                        \n",
       "1599970    Thanks @eastwestchic &amp; @wangyip Thanks! Th...\n",
       "1599971    @marttn thanks Martin. not the most imaginativ...\n",
       "1599972            @MikeJonesPhoto Congrats Mike  Way to go!\n",
       "1599973    http://twitpic.com/7jp4n - OMG! Office Space.....\n",
       "1599974    @yrclndstnlvr ahaha nooo you were just away fr...\n",
       "1599975    @BizCoachDeb  Hey, I'm baack! And, thanks so m...\n",
       "1599976    @mattycus Yeah, my conscience would be clear i...\n",
       "1599977    @MayorDorisWolfe Thats my girl - dishing out t...\n",
       "1599978                            @shebbs123 i second that \n",
       "1599979                                       In the garden \n",
       "1599980    @myheartandmind jo jen by nemuselo zrovna tÃ© ...\n",
       "1599981    Another Commenting Contest! [;: Yay!!!  http:/...\n",
       "1599982    @thrillmesoon i figured out how to see my twee...\n",
       "1599983    @oxhot theri tomorrow, drinking coffee, talkin...\n",
       "1599984    You heard it here first -- We're having a girl...\n",
       "1599985    if ur the lead singer in a band, beware fallin...\n",
       "1599986                @tarayqueen too much ads on my blog. \n",
       "1599987    @La_r_a NEVEER  I think that you both will get...\n",
       "1599988    @Roy_Everitt ha- good job. that's right - we g...\n",
       "1599989                   @Ms_Hip_Hop im glad ur doing well \n",
       "1599990                                WOOOOO! Xbox is back \n",
       "1599991    @rmedina @LaTati Mmmm  That sounds absolutely ...\n",
       "1599992                    ReCoVeRiNg FrOm ThE lOnG wEeKeNd \n",
       "1599993                                    @SCOOBY_GRITBOYS \n",
       "1599994    @Cliff_Forster Yeah, that does work better tha...\n",
       "1599995    Just woke up. Having no school is the best fee...\n",
       "1599996    TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997    Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998    Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999    happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_regex(x):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = X.str.replace(\"@\\S+\", \"\")  # remove name\n",
    "X1 = X1.str.replace(\"#\\S+\", \"\") # remove hashtag\n",
    "\n",
    "X1 = X1.apply(my_regex)  # remove all repeated characters\n",
    "\n",
    "X1 = X1.str.replace(\"\\.\\.\", \" three_dots \") # convert ... to three_dots to be like a word\n",
    "X1 = X1.str.replace(\"\\!\\!\", \" exc_mark \") # convert ... to exc_mark to be like a word\n",
    "\n",
    "X1 = X1.str.replace(\"'t\", \"t\")\n",
    "\n",
    "X1 = X1.str.replace(\"https?://\\S+\", \"\")\n",
    "X1 = X1.str.replace(\"www.\\S+.\\S{2, 4}\", \"\")\n",
    "\n",
    "X1 = X1.str.replace(\"\\s([0-9.,]+)\\s\", \"\") # remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            - Aww, that's a bummer.  You shoulda got Dav...\n",
       "1          is upset that he cant update his Facebook by t...\n",
       "2           I dived many times for the ball. Managed to s...\n",
       "3            my whole body feels itchy and like its on fire \n",
       "4           no, it's not behaving at all. i'm mad. why am...\n",
       "5                                        not the whole crew \n",
       "6                                                Need a hug \n",
       "7           hey  long time no see! Yes three_dots  Rains ...\n",
       "8                                   nope they didnt have it \n",
       "9                                            que me muera ? \n",
       "10         spring break in plain city three_dots  it's sn...\n",
       "11                                I just re-pierced my ears \n",
       "12          I couldnt bear to watch it.  And I thought th...\n",
       "13          It it counts, idk why I did either. you never...\n",
       "14          i would've been the first, but i didnt have a...\n",
       "15          I wish I got to watch it with you exc_mark  I...\n",
       "16         Hollis' death scene will hurt me severely to w...\n",
       "17                                      about to file taxes \n",
       "18          ahh ive always wanted to see rent  love the s...\n",
       "19          Oh dear. Were you drinking out of the forgott...\n",
       "20          i was out most of the day so didnt get much d...\n",
       "21         one of my friend called me, and asked to meet ...\n",
       "22                         I baked you a cake but I ated it \n",
       "23                    this week is not going as i had hoped \n",
       "24                                   blagh class attomorrow \n",
       "25            I hate when I have to call and wake people up \n",
       "26         Just going to cry myself to sleep after watchi...\n",
       "27                                    im sad now  Miss.Lilly\n",
       "28         ooh three_dots  LOL  that leslie three_dots  a...\n",
       "29         Meh three_dots  Almost Lover is the exception ...\n",
       "                                 ...                        \n",
       "1599970    Thanks  &amp;  Thanks! That was just what I wa...\n",
       "1599971     thanks Martin. not the most imaginative inter...\n",
       "1599972                            Congrats Mike  Way to go!\n",
       "1599973     - OMG! Office Space three_dots  I wanna steal...\n",
       "1599974     ahaha noo you were just away from everyone el...\n",
       "1599975      Hey, I'm baack! And, thanks so much for all ...\n",
       "1599976     Yeah, my conscience would be clear in that ca...\n",
       "1599977     Thats my girl - dishing out the &quot;advice&...\n",
       "1599978                                       i second that \n",
       "1599979                                       In the garden \n",
       "1599980      jo jen by nemuselo zrovna tÃ© holce ael co nic \n",
       "1599981      Another Commenting Contest! [;: Yay exc_mark   \n",
       "1599982     i figured out how to see my tweets and facebo...\n",
       "1599983     theri tomorrow, drinking coffee, talking abou...\n",
       "1599984    You heard it here first -- We're having a girl...\n",
       "1599985    if ur the lead singer in a band, beware fallin...\n",
       "1599986                            too much ads on my blog. \n",
       "1599987     NEVEER  I think that you both will get on wel...\n",
       "1599988     ha- good job. that's right - we gotta throw t...\n",
       "1599989                               im glad ur doing well \n",
       "1599990                                   WOO! Xbox is back \n",
       "1599991      Mmm  That sounds absolutely perfect three_do...\n",
       "1599992                    ReCoVeRiNg FrOm ThE lOnG wEeKeNd \n",
       "1599993                                                     \n",
       "1599994     Yeah, that does work better than just waiting...\n",
       "1599995    Just woke up. Having no school is the best fee...\n",
       "1599996    TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997    Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998    Happy 38th Birthday to my boo of all time exc_...\n",
       "1599999                                              happy  \n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Basic\n",
    "\n",
    "First, we will not go more in details in tweets and just apply a TF-IDF and a model to have a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv=TfidfVectorizer(min_df=0, max_features=None, strip_accents='unicode',lowercase =True,\n",
    "                    analyzer='word', token_pattern=r'\\w{3,}', ngram_range=(1,1),\n",
    "                    use_idf=True,smooth_idf=True, sublinear_tf=True, stop_words = \"english\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_transform = tfv.fit_transform(X_train)\n",
    "X_test_transform = tfv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280000, 586147)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our matrice is only with one-gram words and still have a voculary of 586147 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify model and parameters\n",
    "model=LogisticRegression(C=1.)\n",
    "\n",
    "#fit model\n",
    "model.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " auc  0.8956017262751066\n",
      " auc  0.8617762997561205\n",
      "[0.73061013 0.62599728 0.67317318 0.05010197 0.4804858  0.63243171\n",
      " 0.6851576  0.43986609 0.00974582 0.99346639]\n"
     ]
    }
   ],
   "source": [
    "#make prediction on the same (train) data\n",
    "pred_train=model.predict_proba(X_train_transform)[:,1]\n",
    "pred_test=model.predict_proba(X_test_transform)[:,1]\n",
    "\n",
    "#chcek AUC(Area Undet the Roc Curve) to see how well the score discriminates between negative and positive\n",
    "print (\" auc \" , roc_auc_score(y_train//4, pred_train))\n",
    "print (\" auc \" , roc_auc_score(y_test//4, pred_test))\n",
    "\n",
    "#print top 10 scores as a sanity check\n",
    "print (pred_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_weight = np.argsort(model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inversed = { value: key for key, value in tfv.vocabulary_.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad  =>  -12.778844971279486\n",
      "miss  =>  -8.761970325884333\n",
      "sadly  =>  -8.671690392259233\n",
      "poor  =>  -8.170278714517776\n",
      "unfortunately  =>  -7.617925255903621\n",
      "sucks  =>  -7.218759033954542\n",
      "bummed  =>  -7.151845350574452\n",
      "missing  =>  -7.087200029087839\n",
      "gutted  =>  -7.065545068431576\n",
      "sick  =>  -6.889312048800282\n"
     ]
    }
   ],
   "source": [
    "for idx in idx_weight[:10]:\n",
    "    print(inversed[idx], \" => \", model.coef_[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pleasure  =>  4.383097453236274\n",
      "tweeteradder  =>  4.45996142398864\n",
      "smiling  =>  4.53934889692321\n",
      "congrats  =>  4.573540572191535\n",
      "glad  =>  4.665180957066852\n",
      "smile  =>  4.924781976595472\n",
      "congratulations  =>  5.1171661936577895\n",
      "welcome  =>  6.1123945200887855\n",
      "thank  =>  6.463801443258966\n",
      "thanks  =>  6.527945847658128\n"
     ]
    }
   ],
   "source": [
    "for idx in idx_weight[-10:]:\n",
    "    print(inversed[idx], \" => \", model.coef_[0][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a auc quite correct with not a lot of overfitting. Nevertheless, if we look at words with more impact on the prediction, we can see double like thank and thanks or congrats and congratulation. That's why we need a more in-depth preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better pre-processing\n",
    "\n",
    "To do this step, we will use the tweet tokenizer which is better to tokenize tweet which have a more complex sentence grammar. After, to reduce the number of words, we will apply a stemmer to keep only the root of the word and merge words like  \"queries\", \"query\", \"querying\" to the same word \"queri\" in that case. \n",
    "\n",
    "Due to the TweetTokenizer, we may end with 1 ponctuation point or a letter (it tries to keep smileys). We will also remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "def clean(x):\n",
    "    tokens = TweetTokenizer().tokenize(x)\n",
    "    return [SnowballStemmer(\"english\").stem(word) for word in tokens if len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1600000/1600000 [04:30<00:00, 5910.83it/s]\n"
     ]
    }
   ],
   "source": [
    "X_1 = []\n",
    "for sentence in tqdm.tqdm(X1):\n",
    "    X_1.append(clean(sentence))\n",
    "    \n",
    "X1 = X.tolist()  # just for clean check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a lot less words. We can apply the TF-IDF and in addition, we can remove all words which doesn't appears in more than 5 documents. This will end to 30k words. As a result, I add the 2-gram words in the vocabulary. It's slower but provides better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, \n",
    "                        max_features=None, \n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True,\n",
    "                        analyzer='word', \n",
    "                        ngram_range=(1,2),\n",
    "                        use_idf=True, \n",
    "                        smooth_idf=True, \n",
    "                        sublinear_tf=True, \n",
    "                        stop_words = None, #\"english\", \n",
    "                        tokenizer=dummy_fun, \n",
    "                        preprocessor=dummy_fun)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, init_train, init_test = train_test_split(X_1, y, X1, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_transform = tfidf.fit_transform(X_train)\n",
    "X_test_transform = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280000, 295052)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we have 2-grams, our dictionnay is a lot lighter with 295000 words. Now we can train models and fine tune it. \n",
    "\n",
    "I didn't kept all test bu I tried with L1 or L2 penalyt and multiple C factors. The best result is reached with L1-penalty and a C = 0.3. We nearly have no overfitting and the auc above the previous model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.3, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify model and parameters\n",
    "model=LogisticRegression(penalty='l1', C=0.3)\n",
    "\n",
    "#fit model\n",
    "model.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC :  0.8977395289151319\n",
      "Test AUC :  0.8944455770417095\n"
     ]
    }
   ],
   "source": [
    "pred_train=model.predict_proba(X_train_transform)[:,1]\n",
    "pred_test=model.predict_proba(X_test_transform)[:,1]\n",
    "\n",
    "print (\"Train AUC : \" , roc_auc_score(y_train, pred_train))\n",
    "print (\"Test AUC : \" , roc_auc_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate 0.4\n",
      "Train Acc. :  0.80464375\n",
      "Test Acc. :  0.801515625\n",
      "Rate 0.45\n",
      "Train Acc. :  0.81315546875\n",
      "Test Acc. :  0.80950625\n",
      "Rate 0.5\n",
      "Train Acc. :  0.8180921875\n",
      "Test Acc. :  0.8148125\n",
      "Rate 0.55\n",
      "Train Acc. :  0.81872890625\n",
      "Test Acc. :  0.815359375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for t in range(40, 60, 5):\n",
    "    print(\"Rate\", t/100)\n",
    "    print (\"Train Acc. : \" , accuracy_score(y_train, pred_train>t/100 ))\n",
    "    print (\"Test Acc. : \" , accuracy_score(y_test, pred_test>t/100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In term of accuracy, we don't have a very good result. We will check why just after but first, let's look at words and their weight as we did previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_weight = np.argsort(model.coef_[0])\n",
    "inversed = { value: key for key, value in tfidf.vocabulary_.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad  =>  -21.21056124724744\n",
      "miss  =>  -14.716917432255471\n",
      "poor  =>  -12.870395639744682\n",
      "not happi  =>  -12.563533452059742\n",
      "cant  =>  -12.081573217801306\n",
      "sick  =>  -11.88037997943026\n",
      "unfortun  =>  -10.944235784615682\n",
      "depress  =>  -10.607840797101318\n",
      "disappoint  =>  -10.551721093413594\n",
      "hurt  =>  -10.405286120145677\n",
      "hate  =>  -10.38763446225119\n",
      "upset  =>  -10.250304218824947\n",
      "not look  =>  -10.112668100651998\n",
      "wish  =>  -10.07693420054282\n",
      "cri  =>  -10.019714452212156\n",
      "bummer  =>  -9.85788570737082\n",
      "cancel  =>  -9.830784279154846\n",
      "suck  =>  -9.786252314870064\n",
      "broke  =>  -9.585485520110876\n",
      "headach  =>  -9.534027443128409\n",
      "lost  =>  -9.401723711727291\n",
      "rip  =>  -9.229089286813736\n",
      "air franc  =>  -9.224771814291797\n",
      "sadden  =>  -9.207044905850355\n",
      "pass away  =>  -9.104886054315351\n",
      "not good  =>  -8.991351965485975\n",
      "not cool  =>  -8.798331030267612\n",
      "ugh  =>  -8.77816362303622\n",
      "sadd  =>  -8.618030177100175\n",
      "broken  =>  -8.59458566763673\n",
      "lone  =>  -8.470450117633918\n",
      "father day  =>  -8.393575757561562\n",
      "saddest  =>  -8.348713584138153\n",
      "bad  =>  -8.273044425956842\n",
      "missin  =>  -8.220725865157835\n",
      "horribl  =>  -8.20613572225263\n",
      "not fun  =>  -8.204359143084057\n",
      "sold out  =>  -8.02089748394297\n",
      "boo  =>  -7.9984023178477806\n",
      "die  =>  -7.94123446698408\n",
      "funer  =>  -7.936886086508879\n",
      "no  =>  -7.9040370564252855\n",
      "noo  =>  -7.901483582082177\n",
      "fail  =>  -7.696261615939354\n",
      "not nice  =>  -7.666301903850101\n",
      "iran  =>  -7.644679125042084\n",
      "want  =>  -7.639434949264665\n",
      "isnt  =>  -7.455857794759793\n",
      "didnt  =>  -7.443137666530462\n",
      "unfair  =>  -7.413446194449365\n"
     ]
    }
   ],
   "source": [
    "for idx in idx_weight[:50]:\n",
    "    print(inversed[idx], \" => \", model.coef_[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great  =>  5.351644348941758\n",
      "didnt miss  =>  5.3985093250432605\n",
      "pleasur  =>  5.41503465567103\n",
      "couldnt resist  =>  5.44641887737757\n",
      "heheh  =>  5.469564852498508\n",
      "enjoy  =>  5.55688323154081\n",
      "congrat  =>  5.590964171832272\n",
      "noth wrong  =>  5.625265555491154\n",
      "dont miss  =>  5.625571098327797\n",
      "miss me  =>  5.669256543955395\n",
      "no pain  =>  5.677395675773511\n",
      "dont forget  =>  5.7098465281800115\n",
      "nice  =>  5.828553893130498\n",
      "not alon  =>  5.829690939208242\n",
      "dont hate  =>  5.896274749638175\n",
      "no school  =>  5.910846106539679\n",
      "hehe  =>  5.955756071230851\n",
      "dont need  =>  5.9638867686941115\n",
      "cute  =>  5.971062802364617\n",
      "cool  =>  6.009077592413637\n",
      "congratul  =>  6.093079638388798\n",
      "amaz  =>  6.130528656249716\n",
      "no doubt  =>  6.1720584794851465\n",
      "yay  =>  6.2201379739388285\n",
      "never fail  =>  6.238140754977784\n",
      "wont hurt  =>  6.267905717766572\n",
      "never too  =>  6.479048101923512\n",
      "awesom  =>  6.512105550458961\n",
      "not problem  =>  6.564228846598438\n",
      "good  =>  6.757880602348531\n",
      "glad  =>  6.813956091130689\n",
      "love  =>  6.917969396525922\n",
      "excit  =>  7.012052179033998\n",
      "no need  =>  7.013737326551238\n",
      "proud  =>  7.1680357169266475\n",
      "to worri  =>  7.338427927658341\n",
      "welcom  =>  7.3803047464257245\n",
      "thank  =>  7.678684761489882\n",
      "happi  =>  8.145662186004994\n",
      "=(  =>  8.26196364255093\n",
      "no prob  =>  8.845679450094266\n",
      "smile  =>  9.28889159764574\n",
      "cannot wait  =>  9.68972720268853\n",
      "dont worri  =>  9.763674530289258\n",
      "not sad  =>  9.800908142841603\n",
      "doesnt hurt  =>  10.1670147558745\n",
      "not bad  =>  10.680289202658724\n",
      "no problem  =>  12.606565964257802\n",
      "no worri  =>  13.716947335534774\n",
      "cant wait  =>  14.93130643436979\n"
     ]
    }
   ],
   "source": [
    "for idx in idx_weight[-50:]:\n",
    "    print(inversed[idx], \" => \", model.coef_[0][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see double words which is a good point and most words makes sense. The strange one is \"=(\" which is considered as positive. We can see that 2-gram helped to manage more grammar. For example :\n",
    "- good is positive with +6.75\n",
    "- not good is negative with -8.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3194047289395907"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_[0][tfidf.vocabulary_[\"exc_mark\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.071659496958665"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_[0][tfidf.vocabulary_[\"three_dot\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can imagine \"...\" is considered negative and \"!!!\" is considered positive. The second one is less true but should be more a factor as \"yes!!!\" is more positive than \"yes\" and \"no!!!\" is more negative than \"no\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = pred_test - y_test\n",
    "\n",
    "tweet_text = np.array(init_test)\n",
    "\n",
    "index_0 = np.argwhere(y_test == 0)\n",
    "index_1 = np.argwhere(y_test == 1)\n",
    "\n",
    "tweet_0 = tweet_text[index_0]\n",
    "tweet_1 = tweet_text[index_1]\n",
    "\n",
    "my_pred_0 = pred_test[index_0].flatten()\n",
    "my_pred_1 = pred_test[index_1].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check \n",
    "\n",
    "### Success Positive\n",
    "\n",
    "Now let's look at where we are right to predict positive sentiment where is it positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 1.000 - True 1\n",
      "@MGiraudOfficial http://twitpic.com/7epvz - You look great. and happy. smiling is good.  haha. i love your smile.\n",
      "\n",
      "\n",
      "Predict 1.000 - True 1\n",
      "i like smiling \n",
      "\n",
      "\n",
      "Predict 1.000 - True 1\n",
      "@COACHPARSELLS  smiles\n",
      "\n",
      "\n",
      "Predict 1.000 - True 1\n",
      "@Amy_LaRee &quot;&quot;&quot;&quot;SMILES &quot;&quot;&quot;&quot;  \n",
      "\n",
      "\n",
      "Predict 1.000 - True 1\n",
      "@itsuber smile. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(my_pred_1)[-5:]:  # the 5 last index are ones with highest prediction\n",
    "    print(\"Predict {:.03f} - True {}\".format(my_pred_1[i], \"1\"))\n",
    "    print(tweet_1[i][0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success Negative\n",
    "\n",
    "Now, where we are right to predict negative sentiment where is it negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 0.000 - True 0\n",
      "@Elliecopter_  Ellish sad?\n",
      "\n",
      "\n",
      "Predict 0.000 - True 0\n",
      "@RyanSeacrest @vianacoke 9how sad \n",
      "\n",
      "\n",
      "Predict 0.000 - True 0\n",
      "@Nailhead SAD FAEC \n",
      "\n",
      "\n",
      "Predict 0.000 - True 0\n",
      "@teffysnedgehead Sadness. \n",
      "\n",
      "\n",
      "Predict 0.000 - True 0\n",
      "@marissalindh sadness \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(my_pred_0)[:5]:  # the 5 last index are ones with highest prediction\n",
    "    print(\"Predict {:.03f} - True {}\".format(my_pred_0[i], \"0\"))\n",
    "    print(tweet_0[i][0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Positive\n",
    "\n",
    "Now let's look at where we are <b>wrong</b> to predict negative sentiment where is it positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 0.000 - True 1\n",
      "@triciabuck I wish! Airports suck! \n",
      "\n",
      "\n",
      "Predict 0.000 - True 1\n",
      "@GF_Steph I miss my Stephie-poo \n",
      "\n",
      "\n",
      "Predict 0.000 - True 1\n",
      "@LabattBoo i miss my chocobo   ? http://blip.fm/~5ei99\n",
      "\n",
      "\n",
      "Predict 0.000 - True 1\n",
      "@mygaysecrets  I wish!\n",
      "\n",
      "\n",
      "Predict 0.000 - True 1\n",
      "@ScottSharman I wish! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(my_pred_1)[:5]:  # the 5 last index are ones with highest prediction\n",
    "    print(\"Predict {:.03f} - True {}\".format(my_pred_1[i], \"1\"))\n",
    "    print(tweet_1[i][0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there is most probably errors due to the dataset bias. Some tweets are considered positive but they are for some of them clearly negative (first one for example). It has been set as positive most probably because it had a \":)\" at the end (for sarcasm maybe). The 2 last ones are positive and guessed negative due to the weight of wish (-10.07)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Positive\n",
    "\n",
    "Now let's look at where we are <b>wrong</b> to predict positive sentiment where is it negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 1.000 - True 0\n",
      "@DuckDrake Thanks. \n",
      "\n",
      "\n",
      "Predict 1.000 - True 0\n",
      "@jay_f_k thank u \n",
      "\n",
      "\n",
      "Predict 1.000 - True 0\n",
      "@KristianaNKOTB  THANKS\n",
      "\n",
      "\n",
      "Predict 1.000 - True 0\n",
      "@oshidori Thanks. \n",
      "\n",
      "\n",
      "Predict 1.000 - True 0\n",
      "@howbo15 thanks \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(my_pred_0)[-5:]:  # the 5 last index are ones with highest prediction\n",
    "    print(\"Predict {:.03f} - True {}\".format(my_pred_0[i], \"0\"))\n",
    "    print(tweet_0[i][0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's less logical... All of them are \"Thanks\" but labelled as negative. This is clearly a bias of dataset due to use of a smiley. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No sentiment\n",
    "\n",
    "Now let's look at sentences considered without sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive : Predict 0.500 %\n",
      "@Hibippytea  how's ur sat been?\n",
      "\n",
      "\n",
      "Positive : Predict 0.500 %\n",
      "Sitting at Chillies, with folks, waiting on the food. I just want to sleep! \n",
      "\n",
      "\n",
      "Positive : Predict 0.500 %\n",
      "I've gained some weight since I've been in Houston.  \n",
      "\n",
      "\n",
      "Positive : Predict 0.500 %\n",
      "Oh and Brad Roudebush is officially a nerd. Maybe I am just jelous because he is following my Dad, but not me \n",
      "\n",
      "\n",
      "Positive : Predict 0.500 %\n",
      "@pleasurep thee only time we talk is on myspace lol what happened to tweeting me?? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = np.argsort(np.abs(pred_test-0.5))\n",
    "for i in idx[:5]:\n",
    "    print(\"Positive : Predict {:.03f} %\".format(pred_test[i]))\n",
    "    print(init_test[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of them it's just because we have positive and negative words but the sentence have a light balance. For example the second one has:\n",
    "\n",
    "- \"just want\" which is positive\n",
    "- \"waiting\" which is negative\n",
    "\n",
    "### Save for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:/Twitter_data/models/log_reg.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(tfidf, 'F:/Twitter_data/models/tfidf.pkl') \n",
    "joblib.dump(model, 'F:/Twitter_data/models/log_reg.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we learn how to clean tweets and train a model to predict sentiments on a sentence. This will be used on prediction of tweet related to a specific subject later on.\n",
    "\n",
    "We use different tools of NLP (mainly from nltk) to clean the approximative grammar of tweet compare to newspapers for example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
