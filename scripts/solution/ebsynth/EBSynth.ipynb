{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBSynth\n",
    "\n",
    "On a video of 2 minutes paper <a href=\"https://www.youtube.com/watch?v=fcnjHmBcLNQ\" target=\"_blank\">(link)</a>, Károly Zsolnai-Fehér presented a new approch to do a style transfer on a video starting only with one transfered image. \n",
    "\n",
    "Until now, it was required to transform each frame with style transfer. This creates flickering beacuse the transformation starts with random noise. Several approaches were done by using RNN in order to keep temporal consistency but the result was not very good.\n",
    "\n",
    "The new approach can be easily implemented on any project by using the software available for free at <a href=\"https://ebsynth.com/\" target=\"_blank\">ebsynth.com</a>. \n",
    "\n",
    "In this notebook, we will use it to tranfer a part of a video downloaded from youtube. Attention, the video requires to have few movements between frame and no hidden artifact. You should refer to the tutorial on their website to understand it. In this exercice, I'll use 2 videos of a village taken from a drone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video is a complete report about a small village in France. I'll keep only 2 distinct parts with an aerial view taken from a drone.\n",
    "Each frame has to get a numbering and no other info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('video.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "nframes = 1\n",
    "success = True\n",
    "while success:\n",
    "    success, image = vidcap.read()\n",
    "    if 483 < count < 625 or 1510 < count < 1673:\n",
    "        cv2.imwrite(\"input/{:06d}.jpeg\".format(nframes), image)\n",
    "        nframes += 1\n",
    "    if count > 1673:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Style Transfert\n",
    "\n",
    "Now I have all the frame of the 2 parts of the video\n",
    "<ul>\n",
    "    <li>1 to 141 : First part of the video</li>\n",
    "    <li>142 to 303 : Second part of the video</li>\n",
    "</ul>\n",
    "\n",
    "For the Style Transfer, I used a website instead of building the code. I ran it on the first image of the clip and save the rendered image as a keyframe for the tool <a href=\"https://demos.algorithmia.com/deep-style/\" target=\"_blank\">(link)</a>\n",
    "\n",
    "As there is 2 parts in the video, I have to transfer 2 images. They are placed in the folder keys. Attention, the name must match the number of the original image. In that case, the frame 0 and 142 are converted\n",
    "\n",
    "<img src=\"input/000001.jpeg\"/>\n",
    "<img src=\"keys/000001.jpg\"/>\n",
    "<img src=\"input/000142.jpeg\"/>\n",
    "<img src=\"keys/000142.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Transfered video\n",
    "\n",
    "Now we have the video in one folder and the keyframes transfered in another folder. We can now do the rendering of the transfered images\n",
    "\n",
    "<img src=\"EBsynth.png\"/>\n",
    "\n",
    "<b>This step is quite long, it tooks around 1h to render the 300 images</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rebuild the 2 videos (original and transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = []\n",
    "for filename in glob.glob('input/*.jpeg'):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    " \n",
    "out = cv2.VideoWriter('in_video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = []\n",
    "for filename in glob.glob('output/*.png'):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    " \n",
    "out = cv2.VideoWriter('out_video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can be seen on the script page of the website. The result is quite consistent along the video. We can see a small defect on the root of one house because the original video had deinterlacing. However the consistency remains impressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In conclusion, we discovered on this notebook, how to apply style transfer to a video by having only one example of the style transfer. What was not tried is to apply mask. In this software, you can include mask to not transform the background. This will be very usefull in cinema thanks to green background technic but could not be used on this specific video. However, the principle is exactly the same as keyframes.\n",
    "\n",
    "The result is impressive with a software free and easy to use. The temporal consistency is maintain and there is safe bet to say that this method will continue to evolve ,and may be used in animation with latest technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
